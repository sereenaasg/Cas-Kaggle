# -*- coding: utf-8 -*-
"""Cas-Kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18o-V7WS2yRYU2gWgl0QHt1r5lPHRyuJL

Fem els imports necessaris
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import plot_confusion_matrix
from matplotlib.colors import ListedColormap
import matplotlib.patches as mpatches
from sklearn import metrics
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.metrics import ConfusionMatrixDisplay

"""### Coneixent la base de dades

Carreguem la base de dades
"""

# Carreguem dataset rain in Australia
import pandas as pd
pd.set_option('display.float_format', lambda x: '%.3f' % x)

def load_dataset(path):
    dataset = pd.read_csv(path, header=0, delimiter=',')
    return dataset

#dataset = load_dataset('data/Fish.csv')
dataset = load_dataset('Fish.csv')
print("Dimensionalitat de la BBDD:", dataset.shape)

dataset.head()

"""Visualitzem característiques generals sobre els atributs, com el seu tipus i si tenen algun valor NULL."""

dataset.info(verbose=True)

"""### Preparació de la base de dades

Canviem els noms de les columnes LengthX, per noms més representatius
"""

dataset = dataset.rename(columns={"Length1": "len_vertical", "Length2": "len_horitzontal", "Length3": "len_diagonal"})

print("Visualitzem el tipus de cada atribut, el seu domini:")

for i in dataset.columns:
    if dataset[i].dtype == 'object':
        print(f"{i}    \nValues: {dataset[i].unique()}", )
    else:
        print(f"{i}    \nMean_std: {dataset[i].mean():.3f} {dataset[i].std():.3f}", )

"""Mirem les correlacions entre els atributs len_X, ja que poden ser redundants"""

dataset_len = pd.concat([dataset['len_vertical'],dataset['len_horitzontal'], dataset['len_diagonal']], axis= 1)
correlacio = dataset_len.corr()
plt.figure(figsize=(6,5))
ax = sns.heatmap(correlacio, annot=True, linewidths=.5)

"""Per saber quin eliminar, fem correlacio amb Species"""

dataset_species = dataset.copy()
llistaEspecies = ['Bream', 'Roach', 'Whitefish', 'Parkki', 'Perch', 'Pike', 'Smelt']
aux = 1
for especie in llistaEspecies:
    dataset_species.loc[dataset_species.Species == especie, 'Species'] = aux
    aux = aux+1
    
dataset_len['Species'] = dataset_species['Species'].astype('int64')

correlacio = dataset_len.corr()
plt.figure(figsize=(6,5))
ax = sns.heatmap(correlacio, annot=True, linewidths=.5)

"""Creem nou dataset eliminant len_vertical i len_horitzontal del dataset original perque son redundants"""

dataset_def = dataset.copy()
dataset_def = dataset_def.drop('len_vertical', axis = 1)
dataset_def = dataset_def.drop('len_horitzontal', axis = 1)

dataset_def.head()

"""### Preprocessament de dades

#### Tractament de valors nuls

Mirem si tenim algun valor null i la seva distribució:
"""

nuls = dataset.isnull().sum()
print(nuls)

plt.imshow(dataset.isna(), aspect='auto')

"""No tenim cap valor NULL a la base de dades. Mirem els 0's. """

print(dataset_def[dataset_def["len_diagonal"] == 0])
print(dataset_def[dataset_def["Width"] == 0])
print(dataset_def[dataset_def["Height"] == 0])
print(dataset_def[dataset_def["Species"] == 0])
print(dataset_def[dataset_def["Weight"] == 0])

dataset_def[dataset_def["Weight"] == 0]

"""La fila 40 té un 0 a Weigth, es impossible ja que un peix no pot pesar 0 grams. Utilitzem el KNNImputer per inferir aquest valor. Primer passem el 0 a nan i despres usem la funció KNNImputer."""

dataset_def.loc[dataset_def['Weight'] == 0, 'Weight'] = np.nan

for i in dataset_def.columns:
        print(f"{i} \nNaNs: {dataset_def[i].isna().sum()}", )

from sklearn.impute import KNNImputer

dataset_species = dataset_def.copy()
llistaEspecies = ['Bream', 'Roach', 'Whitefish', 'Parkki', 'Perch', 'Pike', 'Smelt']
aux = 1
for especie in llistaEspecies:
    dataset_species.loc[dataset_species.Species == especie, 'Species'] = aux
    aux = aux+1

imputer = KNNImputer(n_neighbors=10, weights="distance")
dataset_def_imp = imputer.fit_transform(dataset_species)

"""Mirem quina distància a inferit i en quina mesura aquesta està encertada"""

dataset_def_imp[40]

dataset_def.fillna(119.6359698, inplace=True)

dataset_def[dataset_def["Weight"] == 119.6359698]

dataset_roach = dataset_def.loc[dataset_def['Species'] == 'Roach']

fig, axes = plt.subplots(figsize=(5,3))
fig.suptitle('Weight espècie Roach', fontsize=10, color = "black")
sns.histplot(data = dataset_roach['Weight']);

"""#### Tractament de dades categòriques

Creem nou dataset amb l'atribut objectiu categoric passat a númeric amb l'estratègia de label encoding
"""

dataset_def_le = dataset_def.copy()
encoder = LabelEncoder()
y = dataset_def_le['Species']
dataset_def_le['Species'] = encoder.fit_transform(dataset_def_le['Species'])
y_mappings_ng = {index: label for index, label in enumerate(encoder.classes_)}
y_mappings_ng

dataset_def['Species']

"""#### Tractament d'outliers

Definim els outliers (valors numèrics de cada atribut del dataset que són molt diferents a la resta de la columna, això pot causar problemes en l'anàlisi de les dades i la posterior creació d'un model que funcioni). Es aconsellable treure'ls abans de continuar amb el preprocessament de dades.

Observem la distribució de les dades on podrem intuir els outliers
"""

num_cols = dataset_def_le.select_dtypes(include="number").columns

for i, col in enumerate(num_cols):
    fig, axes = plt.subplots(figsize=(5,3))
    axes.set_title(col)  
    sns.histplot(data = dataset_def_le[col], kde=True);

num_cols = dataset_def_le.select_dtypes(include="number").columns

for i,col in enumerate(num_cols):
    fig, axes = plt.subplots(figsize=(5,3))
    axes.set_title(col)
    sns.boxplot(data = dataset_def_le[col], orient = "h");

"""Els outliers en aquest cas son els punts que es troben fora dels bigotis del diagrama de caixes, fent una ullada rapida tenim outliers a: weight i a len_diagonal. 
Abans de possar-nos a eliminar aquests outliers, ens interessa saber on trobem aquests outliers. 
"""

print("TRACTAMENT OUTLIERS DE WEIGHT")

Q1 = dataset_def_le['Weight'].quantile(0.25)
Q3 = dataset_def_le['Weight'].quantile(0.75)
IQR = Q3-Q1
mitjana = dataset_def_le['Weight'].median()
minim = dataset_def_le['Weight'].min()
maxim = dataset_def_le['Weight'].max()

print("quartil 1: ", Q1, "     quartil 3: ", Q3, "        rang interquartil: ", IQR)
print("mitjana: ", mitjana, "     valor minim: ", minim, "        valor maxim: ", maxim)

bigoti_inferior = (Q1 - 1.5*IQR)
bigoti_superior = (Q3 + 1.5*IQR)

print(" bigoti inferior: ", bigoti_inferior, "     bigoti superior: ", bigoti_superior)

"""Trobem les files que on es troben els outliers, tenint en compte que només tenim outliers en valors més grans del bigoti superior"""

outliers = (dataset_def_le['Weight'] > bigoti_superior)
print(dataset_def_le[outliers])

"""Els outliers pertanyen tots a la mateixa especie (Pike), analitzarem ara quin és el valor de weight que acostumen a tenir els peixos Pike."""

dataset_plot = dataset_def_le[dataset_def_le['Species'] == 3]

fig, axes = plt.subplots(figsize=(5,3))
axes.set_title('Weight')  
sns.histplot(data = dataset_plot['Weight'], kde=True);

"""Els valors en aquest cas són molt diferents, no sé si es poden considerar outliers. """

print("TRACTAMENT OUTLIERS DE LEN_DIAGONAL")

Q1 = dataset_def_le['len_diagonal'].quantile(0.25)
Q3 = dataset_def_le['len_diagonal'].quantile(0.75)
IQR = Q3-Q1
mitjana = dataset_def_le['len_diagonal'].median()
minim = dataset_def_le['len_diagonal'].min()
maxim = dataset_def_le['len_diagonal'].max()

print("quartil 1: ", Q1, "     quartil 3: ", Q3, "        rang interquartil: ", IQR)
print("mitjana: ", mitjana, "     valor minim: ", minim, "        valor maxim: ", maxim)

bigoti_inferior = (Q1 - 1.5*IQR)
bigoti_superior = (Q3 + 1.5*IQR)

print(" bigoti inferior: ", bigoti_inferior, "     bigoti superior: ", bigoti_superior)

outliers = (dataset_def_le['len_diagonal'] > bigoti_superior)
#outliers = outliers['Weight'] > bigoti_superior
print(dataset_def_le[outliers])

"""L'outlier també és de l'espècie 3"""

dataset_plot = dataset_def_le[dataset_def_le['Species'] == 3]

fig, axes = plt.subplots(figsize=(5,3))
axes.set_title('len_diagonal')  
sns.histplot(data = dataset_plot['len_diagonal'], kde=True);

"""Continuent sent valors molt dispars, no sé si es pot considerar outlier

#### Balancejament de classes

L'únic atribut de tipus object és l'atribut objectiu. Visualitzem de l'atribut objectiu, com estan repartides les mostres.
"""

# Bar Graph of Species 
sns.set_theme(style="darkgrid")

bar_graph = sns.countplot(data = dataset, x= dataset["Species"], order = dataset["Species"].value_counts().index)

unique = dataset["Species"].value_counts()
for i in range(len(unique)):
    bar_graph.text(i, unique[i]/2, str(unique[i]), color ="white", horizontalalignment = "center")

"""{0: 'Bream',
 1: 'Parkki',
 2: 'Perch',
 3: 'Pike',
 4: 'Roach',
 5: 'Smelt',
 6: 'Whitefish'}
 
 Juntem les classes: Smelt, Parkki i Whitefish.
"""

dataset_def.head()

dataset_def_group = dataset_def.copy()
dataset_def_group.loc[dataset_def_group['Species'] == 'Smelt', 'Species'] = 'Other'
dataset_def_group.loc[dataset_def_group['Species'] == 'Parkki', 'Species'] = 'Other'
dataset_def_group.loc[dataset_def_group['Species'] == 'Whitefish', 'Species'] = 'Other'

fig, axes = plt.subplots(figsize=(5,3))
axes.set_title('Species')  
sns.histplot(data = dataset_def_group['Species'], color='purple');

"""Fem label encoding al nou dataset"""

dataset_group_le = dataset_def_group.copy()
encoder = LabelEncoder()
y = dataset_group_le['Species']
dataset_group_le['Species'] = encoder.fit_transform(dataset_group_le['Species'])
y_mappings_g = {index: label for index, label in enumerate(encoder.classes_)}
y_mappings_g

"""Mirem la distribució de les dades"""

sns.pairplot(dataset_def_le, hue="Species", palette="rainbow");

sns.pairplot(dataset_group_le, hue="Species", palette="rainbow");

"""#### Normalitzem les dades"""

dataset_def_le.head()

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

dataset_def_le_norm = dataset_def_le.copy()
dataset_def_le_norm[dataset_def_le_norm.columns] = ss.fit_transform(dataset_def_le_norm[dataset_def_le_norm.columns])  ##nornalitzacio de dades --> mitjana 0, desviació tipica 1

dataset_def_le_norm.head()

dataset_group_le.head()

ss2 = StandardScaler()
dataset_group_le_norm = dataset_group_le.copy()
dataset_group_le_norm[dataset_group_le_norm.columns] = ss.fit_transform(dataset_group_le_norm[dataset_group_le_norm.columns])  ##nornalitzacio de dades --> mitjana 0, desviació tipica 1

dataset_def_le_norm.head()

"""#### PCA

Dataset no agrupat (7 espècies)
"""

pca2 = PCA(n_components=2, random_state=42)
pca_2 = pca2.fit_transform(dataset_def_le_norm[dataset_def_le_norm.columns])

# Creem un dataset amb les dues components principals

df_pca = pd.DataFrame({'PC1' : pca_2[:,0], 'PC2' : pca_2[:,1], 'Species': dataset_def_le_norm['Species']})

pca2.explained_variance_ratio_ # variança de les dues variables
pca2.explained_variance_ratio_.sum() #cuanta variança tenen aquestes dues components principals envers a totes les altres

plt.figure()
plt.title("PCA")
sns.barplot(x=['PC1', 'PC2'], y=pca2.explained_variance_ratio_)

plt.figure(figsize=(10,6))
sns.scatterplot(x='PC1', y='PC2', hue=dataset_def['Species'], data=df_pca)

# no es poden disntingir les clases i amb PCA

"""Les classes, en general, tenen una classificació molt clara en aquest anàlisi, les dues classes que poden generar confusions son: Roach i Pike.

Dataset agrupat 5 espècies
"""

pca2 = PCA(n_components=2, random_state=42)
pca_group = pca2.fit_transform(dataset_group_le_norm[dataset_group_le_norm.columns])

# Creem un dataset amb les dues components principals

df_pca_g = pd.DataFrame({'PC1' : pca_group[:,0], 'PC2' : pca_group[:,1], 'Species': dataset_group_le_norm['Species']})

pca2.explained_variance_ratio_ # variança de les dues variables
pca2.explained_variance_ratio_.sum() #cuanta variança tenen aquestes dues components principals envers a totes les altres

plt.figure()
plt.title("PCA")
sns.barplot(x=['PC1', 'PC2'], y=pca2.explained_variance_ratio_)

plt.figure(figsize=(10,6))
sns.scatterplot(x='PC1', y='PC2', hue=dataset_def_group['Species'], data=df_pca_g)

# no es poden disntingir les clases i amb PCA

"""En aquest cas, veiem que les classes es diferèncien perfectament.

Analitzem si té sentit haver esborrat atributs.
"""

features = dataset_def.drop(['Species'], axis=1)
labels = dataset_def['Species']
st_scaler = StandardScaler()
X = st_scaler.fit_transform(features)
X = pd.DataFrame(X, columns=features.columns)
l_encoder = LabelEncoder()
y = l_encoder.fit_transform(labels)

num_components = X.shape[1]
PC_column_names = ['PC' + str(i) for i in range(1,num_components+1)]
pca = PCA(n_components= num_components)
pca.fit(X)
# pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_
principal_component_variance_plot = plt.figure(figsize=(15,8))
principal_component_variance_plot = sns.barplot(x=explained_variance_ratio, y = PC_column_names,
                                                orient='h', palette='husl')
principal_component_variance_plot.set_title("Variance Explained by Principle components")
principal_component_variance_plot.set_xlabel("Exaplined variance");
principal_component_variance_plot.set_ylabel("Principal component");

features = dataset.drop(['Species'], axis=1)
labels = dataset['Species']
st_scaler = StandardScaler()
X = st_scaler.fit_transform(features)
X = pd.DataFrame(X, columns=features.columns)
l_encoder = LabelEncoder()
y = l_encoder.fit_transform(labels)

num_components = X.shape[1]
PC_column_names = ['PC' + str(i) for i in range(1,num_components+1)]
pca = PCA(n_components= num_components)
pca.fit(X)
# pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_
principal_component_variance_plot = plt.figure(figsize=(15,8))
principal_component_variance_plot = sns.barplot(x=explained_variance_ratio, y = PC_column_names,
                                                orient='h', palette='husl')
principal_component_variance_plot.set_title("Variance Explained by Principle components")
principal_component_variance_plot.set_xlabel("Exaplined variance");
principal_component_variance_plot.set_ylabel("Principal component");

components = pca.components_
print(components)
N =  ['Weight','len_ver','len_hor','len_diag', 'Height','Width']

for i,vector in enumerate(components):
  plt.bar(N,vector)
  plt.title(f'Contribucions de la component {i+1}')

  plt.show()


plt.bar(N,np.sum(np.abs(components),axis = 0))
plt.title(f'Contribucions totals')
plt.show()

# plot a scree plot
components = len(pca.explained_variance_ratio_)  
plt.plot(range(1,components+1), np.cumsum(pca.explained_variance_ratio_ * 100))
plt.xlabel("Number of components")
plt.ylabel("Explained variance (%)")

"""## Selecció de models

### Amb dades no agrupades
"""

features = dataset_def_le_norm.drop(['Species'], axis=1)
labels = dataset_def_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

print(y_test)

"""#### Arbres de decisió"""

print(y_mappings_ng)

for i in range(2,5,1):
  dtree_model = DecisionTreeClassifier(splitter = 'best',max_depth = i)
  dtree_model = dtree_model.fit(X_train, y_train)
  dtree_predictions = dtree_model.predict(X_test)

  acc = dtree_model.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, dtree_predictions, labels=dtree_model.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dtree_model.classes_)
  disp.plot()
  plt.title('Max_depth: {}'.format(i))

  plt.grid(False)
  plt.show()

"""#### SVM"""

from sklearn.svm import SVC

kernel = ['linear', 'poly', 'rbf', 'sigmoid']
for k in kernel:
  svm_model = SVC(kernel = k, C = 0.9, class_weight = 'balanced',  random_state=50)
  svm_model = svm_model.fit(X_train, y_train)
  svm_predictions = svm_model.predict(X_test)

  # model accuracy for X_test 
  acc = svm_model.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, svm_predictions, labels=svm_model.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_model.classes_)
  disp.plot()
  plt.title('Kernel: {}'.format(k))

  plt.grid(False)
  plt.show()

"""#### KNN"""

K = [5,10,20]
for k in K:
  knn = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train)
  knn = knn.fit(X_train, y_train)
  knn_predictions = knn.predict(X_test)

  # model accuracy for X_test 
  acc = knn.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, knn_predictions, labels=knn.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)
  disp.plot()
  plt.title('n_neighbors: {}'.format(k))

  plt.grid(False)
  plt.show()

"""#### Xarxes neuronals"""

layers = [(5,5,5,5), (10,10,10,10), (20,20,20,20)]
for l in layers:
  mlp = MLPClassifier(hidden_layer_sizes=l, max_iter=5000)
  mlp.fit(X_train, y_train)
  predictions = mlp.predict(X_test)

  # model accuracy for X_test 
  acc = mlp.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, predictions, labels=mlp.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)
  disp.plot()
  plt.title('Hidden_layers: {}'.format(l))

  plt.grid(False)
  plt.show()

"""#### Regressió logísitica"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
logreg = LogisticRegression(solver='liblinear', random_state = 0)

# Fit the model
logreg.fit(X_train, y_train)

# Predict data points 
y_pred_test = logreg.predict(X_test)

# Print accuracy scores
print(f'Model accuracy score: {round(accuracy_score(y_test, y_pred_test) * 100, 2)}%')

"""### Amb dades agrupades"""

features = dataset_group_le_norm.drop(['Species'], axis=1)
labels = dataset_group_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(y_test)

"""#### Arbres de decisió"""

print(y_mappings_g)

for i in range(2,5,1):
  dtree_model = DecisionTreeClassifier(splitter = 'best',max_depth = i)
  dtree_model = dtree_model.fit(X_train, y_train)
  dtree_predictions = dtree_model.predict(X_test)

  acc = dtree_model.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, dtree_predictions, labels=dtree_model.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dtree_model.classes_)
  disp.plot()
  plt.title('Max_depth: {}'.format(i))

  plt.grid(False)
  plt.show()

"""#### SVM"""

from sklearn.svm import SVC

kernel = ['linear', 'poly', 'rbf', 'sigmoid']
for k in kernel:
  svm_model = SVC(kernel = k, C = 0.9, class_weight = 'balanced',  random_state=50)
  svm_model = svm_model.fit(X_train, y_train)
  svm_predictions = svm_model.predict(X_test)

  # model accuracy for X_test 
  acc = svm_model.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, svm_predictions, labels=svm_model.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_model.classes_)
  disp.plot()
  plt.title('Kernel: {}'.format(k))

  plt.grid(False)
  plt.show()

"""#### KNN"""

K = [5,10,20]
for k in K:
  knn = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train)
  knn = knn.fit(X_train, y_train)
  knn_predictions = knn.predict(X_test)

  # model accuracy for X_test 
  acc = knn.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, knn_predictions, labels=knn.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)
  disp.plot()
  plt.title('n_neighbors: {}'.format(k))

  plt.grid(False)
  plt.show()

"""#### Xarxes Neuronals"""

layers = [(5,5,5,5), (10,10,10,10), (20,20,20,20)]
for l in layers:
  mlp = MLPClassifier(hidden_layer_sizes=l, max_iter=5000)
  mlp.fit(X_train, y_train)
  predictions = mlp.predict(X_test)

  # model accuracy for X_test 
  acc = mlp.score(X_test, y_test)
  print("Accuracy: ", acc)

  cm = confusion_matrix(y_test, predictions, labels=mlp.classes_)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)
  disp.plot()
  plt.title('Hidden_layers: {}'.format(l))

  plt.grid(False)
  plt.show()

"""#### KNN + xarxes neuronals + svm"""

svm_model = SVC(kernel = 'rbf', C = 0.9, class_weight = 'balanced')
svm_model = svm_model.fit(X_train, y_train)
svm_predictions = svm_model.predict(X_test)

knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train, y_train)
knn = knn.fit(X_train, y_train)
knn_predictions = knn.predict(X_test)

mlp = MLPClassifier(hidden_layer_sizes=(5,5,5,5), max_iter=5000)
mlp.fit(X_train, y_train)
predictions = mlp.predict(X_test)

print("SVM: ", svm_predictions)
print("KNN: ", knn_predictions)
print("MLP: ", predictions)
print("TST: ", y_test)

new_y_test = []
for i in range(0,len(y_test)):
  if knn_predictions[i] == predictions[i]:
    new_y_test.append(predictions[i]) 
  elif svm_predictions[i] == 4:
    new_y_test.append(svm_predictions[i])

  elif knn_predictions[i] != predictions[i] and predictions[i] == svm_predictions[i]:
    new_y_test.append(predictions[i]) 

  elif knn_predictions[i] != predictions[i] and predictions[i] != svm_predictions[i] and svm_predictions[i] == knn_predictions[i]:
    new_y_test.append(svm_predictions[i])

  else:
    new_y_test.append(predictions[i]) 

print("NTS: ", new_y_test)

encerts = 0
for x,i in zip(new_y_test, y_test):
  if x == i:
    encerts = encerts +1
print("Accuracy: ", encerts/len(y_test))
cm = confusion_matrix(y_test, new_y_test, labels=svm_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_model.classes_)
disp.plot()

plt.grid(False)
plt.show()

"""## Optimització d'hyperparàmetres

### Dades no agrupades
"""

features = dataset_def_le_norm.drop(['Species'], axis=1)
labels = dataset_def_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle= True)

print(y_test)

"""#### Arbres de decisió"""

model = DecisionTreeClassifier()

parametres = {'criterion':['gini', 'entropy'],
              'splitter':['best', 'random'],
              'max_depth':[1,2,3,4],
              'max_features':['sqrt', 'log2', None],
              'class_weight':['balanced', None]
}

t_acc_ad_ng = []

for i in range(2,6,1):
  print("k cross-validation: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_ad_ng.append(classifier.best_score_)

"""#### SVM"""

model = SVC()

parametres = {'kernel':['linear', 'poly', 'rbf', 'sigmoid'],
              'gamma':['scale', 'auto'],
              'degree':[1,2,3],
              'decision_function_shape':['ovo', 'ovr'],
              'class_weight':['balanced', None]
}

t_acc_svc_ng = []
for i in range(2,6,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)

  t_acc_svc_ng.append(classifier.best_score_)

"""#### KNN"""

model = KNeighborsClassifier()

parametres = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,17,18,19,20],
              'weights':['uniform', 'distance'],
              'algorithm':['auto', 'ball_tree', 'kd_tree','brute']
}

t_acc_knn_ng = []
for i in range(2,6,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_knn_ng.append(classifier.best_score_)

"""#### Xarxes Neuronals"""

model = MLPClassifier()

parametres = {
              'hidden_layer_sizes': [(5,5,5), (5,5,7), (5,5,5,7), (5,5,5,5), (10,10,10), (10,10,7), (10,10,10,7), (10,10,10,10)],
              'activation':['identity', 'logistic', 'tanh', 'relu'],
              'solver': [ 'sgd', 'adam'],
              'max_iter':[5000,10000]


}
t_acc_mlp_ng = []

t_acc_mlp_ng = []

for i in range(2,6,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_mlp_ng.append(classifier.best_score_)

"""#### Representació"""

data = [t_acc_ad_ng, t_acc_svc_ng, t_acc_knn_ng, t_acc_mlp_ng] 
  
fig = plt.figure(figsize =(8, 6)) 
  
ax = fig.add_axes([0, 0, 1, 1]) 
bp = ax.boxplot(data) 

plt.ylabel('Accuracy')
plt.xlabel('Model')
plt.yticks([0.5,0.6,0.7,0.8,0.9,1])
ax.set_xticklabels(['Arbre', 'SVM', 'KNN', 'XXNN']) 
plt.title('Accuracy amb diferents models')
  
plt.show()

"""### Dades agrupades"""

features = dataset_group_le_norm.drop(['Species'], axis=1)
labels = dataset_group_le['Species']


X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

print(y_test)

"""#### Arbres de decisió"""

model = DecisionTreeClassifier()

parametres = {'criterion':['gini', 'entropy'],
              'splitter':['best', 'random'],
              'max_depth':[1,2,3,4],
              'max_features':['sqrt', 'log2', None],
              'class_weight':['balanced', None]
}

t_acc_ad_g = []
for i in range(2,11,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_ad_g.append(classifier.best_score_)

"""#### SVM"""

model = SVC()

parametres = {'kernel':['linear', 'poly', 'rbf', 'sigmoid'],
              'gamma':['scale', 'auto'],
              'degree':[1,2,3],
              'decision_function_shape':['ovo', 'ovr'],
              'class_weight':['balanced', None]
}

t_acc_svc_g = []

for i in range(2,11,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_svc_g.append(classifier.best_score_)

"""#### KNN"""

model = KNeighborsClassifier()

parametres = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,17,18,19,20],
              'weights':['uniform', 'distance'],
              'algorithm':['auto', 'ball_tree', 'kd_tree','brute'],
}

t_acc_knn_g = []

for i in range(2,11,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_svc_g.append(classifier.best_score_)

"""#### Xarxes neuronals"""

model = MLPClassifier()

parametres = {
              'hidden_layer_sizes': [(5,5,5),(5,5,5,5), (10,10,10),(10,10,5),(10,10,10,5),(10,10,10,10)],
              'activation':['identity', 'logistic', 'tanh', 'relu'],
              'solver': [ 'sgd', 'adam'],
              'max_iter':[5000,10000]


}

t_acc_mlp_g = []
for i in range(2,11,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  t_acc_mlp_g.append(classifier.best_score_)

"""#### Representació"""

data = [t_acc_ad_ng, t_acc_svc_ng, t_acc_knn_ng, t_acc_mlp_ng] 
  
fig = plt.figure(figsize =(8, 6)) 
  
ax = fig.add_axes([0, 0, 1, 1]) 
bp = ax.boxplot(data) 

plt.ylabel('Accuracy')
plt.xlabel('Model')
plt.yticks([0.5,0.6,0.7,0.8,0.9,1])
ax.set_xticklabels(['Arbre', 'SVM', 'KNN', 'XXNN']) 
plt.title('Accuracy amb diferents models')
  
plt.show()

"""## Lazy Predictor"""

try:
    import lazypredict
except:
    !pip install lazypredict;
finally:
    from lazypredict.Supervised import LazyClassifier

##no agrupades
features = dataset_def_le_norm.drop(['Species'], axis=1)
labels = dataset_def_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, 
                                                    random_state=100)

clf = LazyClassifier(ignore_warnings = True, custom_metric = None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

models.sort_values('Accuracy', ascending=False)

##agrupades

features = dataset_group_le_norm.drop(['Species'], axis=1)
labels = dataset_group_le['Species']


X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, 
                                                    random_state=100)

clf = LazyClassifier(ignore_warnings = True, custom_metric = None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

models.sort_values('Accuracy', ascending=False)

features = dataset.drop(['Species'], axis=1)
labels = dataset['Species']


X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, 
                                                    random_state=100)

clf = LazyClassifier(ignore_warnings = True, custom_metric = None)
models, predictions = clf.fit(X_train, X_test, y_train, y_test)

models.sort_values('Accuracy', ascending=False)

"""why? xd

### QuadraticAnalisys
"""

accuracy = []

features = dataset_def_le_norm.drop(['Species'], axis=1)
labels = dataset_def_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, 
                                                    random_state=100)

clf = QuadraticDiscriminantAnalysis()
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)

acu = metrics.accuracy_score(y_test,y_pred)
print(acu)
accuracy.append(acu)

"""Ara aplicarem GridSearch per veure si podem millorar aquesta accuracy"""

model = QuadraticDiscriminantAnalysis()

parametres = {'reg_param':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],
              'tol':[1.0e-4,1.0e-3,1.0e-2],
}

for i in range(2,11,1):
  print("cv: ", i)
  classifier = GridSearchCV(model, parametres, cv=i)
  classifier.fit(X_train,y_train)
  print(classifier.best_params_)
  print(classifier.best_score_)
  accuracy.append(classifier.best_score_)

print(accuracy)

plt.plot([1,2,3,4,5,6,7,8,9,10], accuracy, marker="o")
plt.ylabel('Accuracy')
plt.xlabel('K')
plt.title('Accuracy en funció de k a cross-validation')
plt.xticks([1,2,3,4,5,6,7,8,9,10],[1,2,3,4,5,6,7,8,9,10])
plt.show()

"""## Comparació amb altres treballs

https://www.kaggle.com/code/mayurkagathara/multiple-classification-models

##### Regressió Logística
"""

features = dataset_def_le_norm.drop(['Species'], axis=1)
labels = dataset_def_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2)
print(y_test)

model_logistic_regression = LogisticRegression()
model_logistic_regression.fit(X_train, y_train)
print("Accuracy: ", round(model_logistic_regression.score(X_test, y_test),4))

print(y_mappings_ng)

plot_confusion_matrix(model_logistic_regression, X_train, y_train);
plt.title('Paràmetres per defecte')
plt.grid(False)

model_logistic_regression_2 = LogisticRegression(penalty='none')
model_logistic_regression_2.fit(X_train, y_train)
print("Accuracy: ", round(model_logistic_regression_2.score(X_test, y_test),4))

print(y_mappings_ng)
plot_confusion_matrix(model_logistic_regression_2, X_train, y_train);
plt.title('Penalty: None')
plt.grid(False)

model_logistic_regression_3 = LogisticRegression(C=0.0,penalty='none', class_weight='balanced')
model_logistic_regression_3.fit(X_train, y_train)
model_logistic_regression_3.score(X_test, y_test)

print(y_mappings_ng)
plot_confusion_matrix(model_logistic_regression_3, X_train, y_train);
plt.title('Class_weight: Balanced')
plt.grid(False)

parameters = {'penalty':['l2', 'l1'], 'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], 'class_weight':['balanced', None]}
lr = LogisticRegression(solver='liblinear')
estimators = []
scores = []

for i in range(0,10):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  clf = GridSearchCV(lr, parameters, cv=4)
  clf.fit(X_train, y_train)
  print('Best_estimator: ',clf.best_estimator_)
  print('Best Score: ', clf.best_score_)
  estimators.append(clf.best_estimator_)
  scores.append(clf.best_score_)

plt.plot([1,2,3,4,5,6,7,8,9,10], scores, marker="o")
plt.ylabel('Accuracy')
plt.xlabel('iteracio')
plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])
plt.title('Best Scores en diferents iteracions')
plt.xticks([1,2,3,4,5,6,7,8,9,10],[1,2,3,4,5,6,7,8,9,10])
plt.show()

"""Fem feature impportance"""

import math

features = dataset_group_le_norm.drop(['Species'], axis=1)
labels = dataset_group_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LogisticRegression()
model.fit(X_train, y_train)

print("Accuracy: ", round(model.score(X_test, y_test),4))

w0 = model.intercept_[0]
w = w1, w2, w3, w4 = model.coef_[0]
feature_names = ['weight', 'len_diagonal', 'Height', 'Width']
feature_importance = pd.DataFrame(feature_names, columns = ["feature"])
feature_importance["importance"] = pow(math.e, w)
feature_importance = feature_importance.sort_values(by = ["importance"], ascending=False)
 
ax = feature_importance.plot.barh(x='feature', y='importance')
plt.show()

pca2 = PCA(n_components=2, random_state=42)
pca_group = pca2.fit_transform(dataset_group_le_norm[dataset_group_le_norm.columns])

# Creem un dataset amb les dues components principals

df_pca_g = pd.DataFrame({'PC1' : pca_group[:,0], 'PC2' : pca_group[:,1], 'Species': dataset_group_le_norm['Species']})

pca2.explained_variance_ratio_ # variança de les dues variables
pca2.explained_variance_ratio_.sum() #cuanta variança tenen aquestes dues components principals envers a totes les altres

plt.figure()
plt.title("PCA")
sns.barplot(x=['PC1', 'PC2'], y=pca2.explained_variance_ratio_)

plt.figure(figsize=(10,6))
sns.scatterplot(x='PC1', y='PC2', hue=dataset_def_group['Species'], data=df_pca_g)

"""Entrenem el dataset següent formar amb les dues components principals del PCA"""

df_pca_g

features = df_pca_g.drop(['Species'], axis=1)
labels = dataset_group_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  
model_logistic_regression_pca = LogisticRegression(class_weight='balanced')
model_logistic_regression_pca.fit(X_train, y_train)
model_logistic_regression_pca.score(X_test, y_test)
pca_predictions = model_logistic_regression_pca.predict(X_test)

acc = model_logistic_regression_pca.score(X_test, y_test)
print("Accuracy: ", acc)

cm = confusion_matrix(y_test, pca_predictions, labels=model_logistic_regression_pca.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_logistic_regression_pca.classes_)
disp.plot()
plt.title('PCA model')

plt.grid(False)
plt.show()

pca = PCA(n_components=2, random_state=42)
pca_def = pca.fit_transform(dataset_def_le_norm[dataset_def_le_norm.columns])

# Creem un dataset amb les dues components principals

df_pca = pd.DataFrame({'PC1' : pca_def[:,0], 'PC2' : pca_def[:,1], 'Species': dataset_def_le['Species']})

pca.explained_variance_ratio_ # variança de les dues variables
pca.explained_variance_ratio_.sum() #cuanta variança tenen aquestes dues components principals envers a totes les altres

plt.figure()
plt.title("PCA")
sns.barplot(x=['PC1', 'PC2'], y=pca.explained_variance_ratio_)

plt.figure(figsize=(10,6))
sns.scatterplot(x='PC1', y='PC2', hue=dataset_def['Species'], data=df_pca)

df_pca

features = df_pca.drop(['Species'], axis=1)
labels = dataset_def_le['Species']

X = features.values
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  
model_logistic_regression_pca = LogisticRegression(class_weight='balanced')
model_logistic_regression_pca.fit(X_train, y_train)
model_logistic_regression_pca.score(X_test, y_test)
pca_predictions = model_logistic_regression_pca.predict(X_test)

acc = model_logistic_regression_pca.score(X_test, y_test)
print("Accuracy: ", acc)

cm = confusion_matrix(y_test, pca_predictions, labels=model_logistic_regression_pca.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_logistic_regression_pca.classes_)
disp.plot()
plt.title('PCA model')

plt.grid(False)
plt.show()